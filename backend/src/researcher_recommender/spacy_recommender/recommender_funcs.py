import pandas as pd
import copy
import numpy as np



## remove all numbers from a string
def remove_numbers(string):
    return ''.join([s for s in string if not s.isdigit()])

## removes specfic words that show up in publications titles


def remove_specific_words(string):
    strings_to_remove = ['pp']

    return ''.join([s for s in string if s not in strings_to_remove])


# removes any white spacing from a string
def remove_unwanted_spacing(doc):
    doc_str = str(doc)
    # doc_ls = doc_str.split(" ")
    doc_ls = doc_str.split()

    doc_ls_clean = [word.strip(' ') for word in doc_ls]
#     print(doc_ls_clean)

    empty_str = ''
    while empty_str in doc_ls_clean:
        doc_ls_clean.remove(empty_str)

    clean_str = ' '.join(doc_ls_clean)

    return clean_str


def process_text_for_tdidf(string):
    funcs = [remove_unwanted_spacing, remove_numbers, remove_specific_words]

    for f in funcs:
        string = f(string)

    return string


# Uses keyBERT model to extract keywords from a project's description
# Threshold parameter is used to filter out irrelevant keyphrases (is compared against the
# relevance score generated by keyBERT model)
def extract_keywords_from_project(nlp, kw_model, title, short_desc, long_desc, threshold):
    # top 2 keyphrases retribed 
    title_keywords = kw_model.extract_keywords(process_text_for_tdidf(title),
                                               keyphrase_ngram_range=(1, 3), stop_words='english',
                                               use_mmr=True, diversity=0.6,
                                               top_n=2)
    desc = short_desc + long_desc
    desc_keywords = kw_model.extract_keywords(process_text_for_tdidf(desc),
                                              keyphrase_ngram_range=(1, 3), stop_words='english',
                                              use_mmr=True, diversity=0.6,
                                              top_n=18)

    project_keywords = title_keywords
    project_keywords.extend(desc_keywords)


    project_keywords.sort(key=lambda tup: tup[1], reverse=True)

    if len(project_keywords) == 0:
        return project_keywords

    # standardised the keyphrase weightings by setting the top keyphrase to 100
    max_val = project_keywords[0][1]
    project_keywords_perc = []

    for words, val in project_keywords:
        if val/max_val > threshold: # filter by threshold parameter
            project_keywords_perc.append(((nlp(words), val, val/max_val)))


    return project_keywords_perc

# Accepts fingerprint: list of tuples
# Returns a nlp Doc object where all the keywords are aggregated in one string 
def get_list_of_fingerprint_keywords(nlp, fingerprint):
    ls = [nlp_keywords.text for nlp_keywords, val, perc in fingerprint]

    fingerprint_keywords = ' '.join(ls)

    return nlp(fingerprint_keywords)

# Accepts pandas df column which stores the 'fingerprint' of each researcher
# Returns pandas df column which stores the keyphrases in each fingerprint in a csv format
def convert_fingerprints_csv(fingerprint_col):
    new_col = []

    for fingerprint in fingerprint_col:
        string = ''

        for keywords, val, perc in fingerprint:
            string += keywords.text
            string += ', '

        new_col.append(string.rstrip(", "))

    return new_col

def convert_fingerprint_ls_dict(fingerprint, threshold):
    ls = []

    # filter out keywords based on threshold value
    for keywords, val, perc in fingerprint:
        if perc > threshold:
            ls.append({'keywords': keywords.text, 'score': perc*100})

    return ls

# Project_fingerprint and researcher_fingerprint are list of tuples: [(keyphrase string, keyBERT weight, perc weight), ]
# project_fingerprint_str, researcher_fingerprint_str: string of keywords in fingerprints 
# Returns a single ranking score for a researcher
def calc_ranking_scores_researcher_project(project_fingerprint, project_fingerprint_str,
                                           researcher_fingerprint, researcher_fingerprint_str):
    scores = []

    relevance_threshold = 0.4

    # computing all weighted pairwise similarity scores between researcher and project 
    for proj_keywords, proj_val, proj_perc, in project_fingerprint:
        if proj_perc > relevance_threshold:
            for res_keywords, res_val, res_perc, in researcher_fingerprint:
                if res_perc > relevance_threshold:
                    scores.append(proj_keywords.similarity(
                        res_keywords)*proj_val)  

    scores = np.array(scores)
    score1 = np.median(scores) # take median of pairwise scores

    score2 = project_fingerprint_str.similarity(researcher_fingerprint_str)

    return score1*0.80 + 0.20*score2

# Returns a top N researchers for a particular project
# profiles: pandas df of researcher profiles
def get_top_N_researchers_fingerprint_method(N, nlp, kw_model, project_fingerprint, profiles, title, short_desc, long_desc, data_format="df"):
    profiles["Similarity"] = 0 #initialise scores to zero
    project_keywords_nlp = get_list_of_fingerprint_keywords(nlp, project_fingerprint)

    for index, row in profiles.iterrows():
        score = calc_ranking_scores_researcher_project(project_fingerprint, project_keywords_nlp,
                                                       row['Fingerprint'], row['KeywordsString'])

        profiles.loc[index, "Similarity"] = score

    profiles = profiles.sort_values(by=["Similarity"], axis=0, ascending=False)

    top_N = None
    cols_to_pick = ['Name', "Email", "Bio", "Organisation",
                         "Url", "FingerprintCsv"]
                         
    profiles["FingerprintCsv"] = convert_fingerprints_csv(
            profiles["Fingerprint"])

    top_N = copy.deepcopy(profiles.iloc[0:N][cols_to_pick])
    top_N = top_N.reset_index(drop=True)
    
    # formatting dataframe in a json format (to be sent to the React frontend)
    if data_format == "json":        
        # remove_nlp_objects(top_N)
        top_N = top_N.T.to_json()


    return top_N
